{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go-ZT 10-fold cross validation\n",
    "### This notebook contains the code need to run a random 10 fold cross validation of a regression trained DNN using [0,1,2] encoded summary toxicity matrices and chemicals structural data.\n",
    "#### See http://biorxiv.org/lookup/doi/10.1101/2020.10.02.322917 for details\n",
    "\n",
    "By Adrian J Green, PhD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version 2.1.0 . Executing eagerly? True\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "\n",
      "Number of GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"           # Only one GPU will be seen\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "\n",
    "print(\"tensorflow version\",tf.__version__,\". Executing eagerly?\",tf.executing_eagerly())\n",
    "\n",
    "# minimize GPU useage by allowing memeory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "print(\"\\nNumber of GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import pathlib\n",
    "import collections\n",
    "import warnings\n",
    "import timeit\n",
    "\n",
    "# plotting, especially for jupyter notebooks\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from IPython.display import Image\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# local routines\n",
    "from chemdataprep import load_PDBs\n",
    "from toxmathandler_AG import load_tmats\n",
    "\n",
    "# NN build routines\n",
    "from NNbuild_train_vis import init_generator\n",
    "\n",
    "# NN train routines\n",
    "from NNbuild_train_vis import write_training_file\n",
    "\n",
    "# Performance evaluation routines\n",
    "from gen_AggE import calc_AggE, display_conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global data options\n",
    "\n",
    "### PDB options\n",
    "\n",
    "# cGAN & views parameters\n",
    "# [Gfeatures,Gbaselayers,Glayers,Dfeatures,Dbaselayers,Dlayers,carbonbased, setNatoms, views, ClassLabels]\n",
    "parameters = [279, 3, 11, 18, 0, 0, False, 82, 126, None] # Go-ZT\n",
    "\n",
    "## Option to base views on carbon or not. (Safe even if some have no carbon.)\n",
    "# Setting True will make the data smaller in memory and everything run faster.\n",
    "carbonbased = parameters[6]\n",
    "## Option for truncating the length of views.\n",
    "# Truncating will make the data and NN smaller and things run faster.\n",
    "# It make sense if we believe that looking at all neighborhoods of some size \n",
    "# gives sufficient understanding of the chemical.\n",
    "# setNatoms = None # use max number in data\n",
    "setNatoms = parameters[7] # truncate to this number\n",
    "parameters[9] = None # allow cGAN to use class labels in training, None or int\n",
    "\n",
    "views=parameters[8]\n",
    "\n",
    "dataType = '(0,1)_18x1'\n",
    "\n",
    "if (dataType.find('(0,1)_18x6')!=-1):\n",
    "    concentrations = [0,1,2,3,4,5]\n",
    "else:\n",
    "    concentrations = [5]    ## Which of the available endpoints to use\n",
    "endpoints = [i for i in range(4,22)] # use all\n",
    "    \n",
    "genpath = 'AG-model-GT-'+dataType+'.h5'\n",
    "discpath = 'AG-model-DT-'+dataType+'.h5'\n",
    "\n",
    "# Traning individual toxic\n",
    "trpath = '/home2/ajgreen4/Read-Across_w_GAN/DataFiles/(0,1,2)_encoding/Tox21_training_compounds/'\n",
    "valpath = '/home2/ajgreen4/Read-Across_w_GAN/DataFiles/(0,1,2)_encoding/Tox21_validation_compounds/'\n",
    "allpath = '/home2/ajgreen4/Read-Across_w_GAN/DataFiles/(0,1,2)_encoding/tox21_all_compounds/'\n",
    "    \n",
    "modelpath = '/home2/ajgreen4/Read-Across_w_GAN/Models/'\n",
    "dataType = '(0,1)_18x1'\n",
    "genPrefix = 'AG-model-regGT-'+dataType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "* We may want more info, such as a charge, to be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003 pdb files found at /home2/ajgreen4/Read-Across_w_GAN/DataFiles/(0,1,2)_encoding/tox21_all_compounds/PDBs/\n",
      "Species occurring = {'F', 'B', 'P', 'C', 'BR', 'S', 'N', 'O', 'SI', 'I', 'CL', 'H', 'AS'}\n",
      "Setting all views to Natoms= 82\n",
      "126 views needed, but setting to 126\n",
      "Maximum views used = 126\n",
      "Data tensor (w,v) shapes= (1003, 126) (1003, 126, 410)\n"
     ]
    }
   ],
   "source": [
    "# load all files files\n",
    "[ws, vs, Natoms, Nviews, chemnames, Vshape] = load_PDBs(allpath,setNatoms=setNatoms,setNviews=views, carbonbased=carbonbased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using toxicity matrix as labels\n",
      "Transforming encoding to [no effect,effect or NA (dead)]=[0,1].\n",
      "Number of chemicals= 1003\n",
      "Using concentrations ['64 uM']\n",
      "Using endpoints: ['MORT', 'YSE_', 'AXIS', 'EYE_', 'SNOU', 'JAW_', 'OTIC', 'PE__', 'BRAI', 'SOMI', 'PFIN', 'CFIN', 'PIG_', 'CIRC', 'TRUN', 'SWIM', 'NC__', 'TR__']\n",
      "Toxicity vector length Ntoxicity= 18\n",
      "Using 1 concentrations\n",
      "Using 18 endpoints\n"
     ]
    }
   ],
   "source": [
    "# outputs/labels\n",
    "# toxicity\n",
    "print(\"Using toxicity matrix as labels\")\n",
    "### Toxicity matrix options\n",
    "if (genPrefix.find('(0,1)_18x6')!=-1):\n",
    "    concentrations = [0,1,2,3,4,5]\n",
    "else:\n",
    "    concentrations = [5]    ## Which of the available endpoints to use\n",
    "endpoints = [i for i in range(4,22)] # use all\n",
    "[toxicity,rows, cols, fish] = load_tmats(allpath,chemnames, \n",
    "                                                    concentration_indexes=concentrations,\n",
    "                                                    endpoint_indexes=endpoints, transform=True,\n",
    "                                                    verbose=1)\n",
    "\n",
    "# legend labels for plotting\n",
    "endpoints = [i for i in range(len(rows))]\n",
    "concentrations = [i for i in range(len(cols))]\n",
    "print(\"Using\", len(concentrations), \"concentrations\")\n",
    "print(\"Using\", len(endpoints), \"endpoints\")\n",
    "legend = [rows,cols,endpoints,concentrations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting generator with conventional regression\n",
    "\n",
    "This tests if G has enough power to get the truth, how fast it could learn, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses and optimizers\n",
    "\n",
    "# losses\n",
    "Gloss_function = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# optimizers\n",
    "generator_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Initialize RegGen\n",
    "generator = init_generator([ws, vs],toxicity,parameters)\n",
    "                           \n",
    "# Shuffle split chemicals\n",
    "ss = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold :  1  of 10\n",
      "Validation Dataset\n",
      "    Kappa:  0.598   AUROC:  0.754     SE 52.6\n",
      "Train loss: 0.0221  Validation loss: 0.0207\n",
      "Time taken for fold: 6.8 \n",
      "\n",
      "Running fold :  2  of 10\n",
      "Validation Dataset\n",
      "    Kappa:  0.463   AUROC:  0.6909     SE 41.2\n",
      "Train loss: 0.0223  Validation loss: 0.02\n",
      "Time taken for fold: 7.1 \n",
      "\n",
      "Running fold :  3  of 10\n",
      "Validation Dataset\n",
      "    Kappa:  0.392   AUROC:  0.6735     SE 40.6\n",
      "Train loss: 0.0195  Validation loss: 0.0311\n",
      "Time taken for fold: 7.4 \n",
      "\n",
      "Running fold :  4  of 10\n",
      "Validation Dataset\n",
      "    Kappa:  0.425   AUROC:  0.6645     SE 34.1\n",
      "Train loss: 0.0222  Validation loss: 0.0204\n",
      "Time taken for fold: 6.7 \n",
      "\n",
      "Running fold :  5  of 10\n",
      "Validation Dataset\n",
      "    Kappa:  0.499   AUROC:  0.7056     SE 43.6\n",
      "Train loss: 0.0201  Validation loss: 0.0287\n",
      "Time taken for fold: 7.3 \n",
      "\n",
      "Running fold :  6  of 10\n",
      "Validation Dataset\n",
      "    Kappa:  0.594   AUROC:  0.7506     SE 51.9\n",
      "Train loss: 0.0227  Validation loss: 0.0182\n",
      "Time taken for fold: 6.9 \n",
      "\n",
      "Running fold :  7  of 10\n",
      "Validation Dataset\n",
      "    Kappa:  0.432   AUROC:  0.6716     SE 36.7\n",
      "Train loss: 0.0224  Validation loss: 0.0194\n",
      "Time taken for fold: 7.0 \n",
      "\n",
      "Running fold :  8  of 10\n",
      "Validation Dataset\n",
      "    Kappa:  0.593   AUROC:  0.7578     SE 53.8\n",
      "Train loss: 0.0228  Validation loss: 0.0179\n",
      "Time taken for fold: 7.5 \n",
      "\n",
      "Running fold :  9  of 10\n",
      "Validation Dataset\n",
      "    Kappa:  0.49   AUROC:  0.7287     SE 51.6\n",
      "Train loss: 0.0217  Validation loss: 0.0225\n",
      "Time taken for fold: 7.5 \n",
      "\n",
      "Running fold :  10  of 10\n",
      "Validation Dataset\n",
      "    Kappa:  0.462   AUROC:  0.6876     SE 40.0\n",
      "Train loss: 0.022  Validation loss: 0.0213\n",
      "Time taken for fold: 8.3 \n",
      "\n",
      "CPU times: user 1min 5s, sys: 6.7 s, total: 1min 11s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 1\n",
    "# fit\n",
    "for train_index, test_index in ss.split(ws): \n",
    "    start_time = timeit.default_timer()\n",
    "    print(\"Running fold : \", k, \" of 10\")\n",
    "    X_train = [ws[train_index], vs[train_index]]\n",
    "    Y_train = toxicity[train_index]\n",
    "\n",
    "    X_test = [ws[test_index], vs[test_index]]\n",
    "    Y_test = toxicity[test_index]\n",
    "    \n",
    "    BATCH_SIZE = 201\n",
    "    history = generator.fit(X_train,Y_train,\n",
    "                  epochs=75,batch_size=BATCH_SIZE,verbose=0,\n",
    "                  validation_data=(X_test, Y_test))\n",
    "    training_loss = round(generator.evaluate(X_train,Y_train,verbose=0), 4)\n",
    "    validation_loss = round(generator.evaluate(X_test,Y_test,verbose=0), 4)\n",
    "    \n",
    "    gen_lab = generator.predict(X_test)\n",
    "    chemnames_test = [chemnames[i] for i in test_index]\n",
    "    \n",
    "    # Calculate chemical activity - ignoring warning due to potential division by zero\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(action='ignore', category=Warning)\n",
    "        [gen_activity_table, tox_activity_table, gen_AggE, tox_AggE] = calc_AggE(Y_test, chemnames_test, \n",
    "                                                                                 gen_lab, fish[test_index], \n",
    "                                                                                 endpoints, concentrations, \n",
    "                                                                                 verbose=0)\n",
    "\n",
    "        metrics = display_conf_matrix(gen_activity_table, tox_activity_table, Gmodelname='Go-ZT') \n",
    "        \n",
    "    print(\"Validation Dataset\")\n",
    "    print('    Kappa: ', metrics[0], '  AUROC: ', metrics[1], '    SE', metrics[2])\n",
    "    print(\"Train loss:\", training_loss, \" Validation loss:\", validation_loss)\n",
    "    \n",
    "    model_ID = \"AG-model-GT-\"+dataType+\"-Kappa-\"+str(metrics[0])+\"-\"+str(k)+\"-fold.h5\"\n",
    "\n",
    "    summary_file_df = write_training_file(parameters,[model_ID, concentrations, ws.shape[1], training_loss, \n",
    "                                                      validation_loss], metrics, \n",
    "                                          '/home2/ajgreen4/Read-Across_w_GAN/output/test.xlsx')\n",
    "    \n",
    "    # Re-initialize the RegGen\n",
    "    generator = init_generator([ws, vs],toxicity,parameters)\n",
    "    k += 1\n",
    "    \n",
    "    # Determine time taken to run fold\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"Time taken for fold:\", round(elapsed, 1), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
